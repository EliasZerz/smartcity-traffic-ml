{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67cfe0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:56.364145Z",
     "iopub.status.busy": "2025-12-03T22:08:56.363959Z",
     "iopub.status.idle": "2025-12-03T22:08:57.265813Z",
     "shell.execute_reply": "2025-12-03T22:08:57.264908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: SUPERVISED MACHINE LEARNING (REGRESSION)\n",
      "======================================================================\n",
      "\n",
      "All required libraries loaded successfully!\n",
      "Timestamp: 2025-12-04 00:30:12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, \n",
    "                             mean_absolute_percentage_error)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: SUPERVISED MACHINE LEARNING (REGRESSION)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll required libraries loaded successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67e769",
   "metadata": {},
   "source": [
    "# Phase 2: Supervised Machine Learning - Complete Regression Analysis\n",
    "\n",
    "**Team Member:** Omar Khaled  \n",
    "**Objective:** Build and evaluate multiple regression models for price prediction with comprehensive analysis\n",
    "\n",
    "## Overview\n",
    "This notebook implements:\n",
    "- 6 regression models (XGBoost, Random Forest, Gradient Boosting, Linear Regression, Ridge, Lasso)\n",
    "- Advanced metrics (RMSE, MAE, R², MAPE, Quantile Loss)\n",
    "- Spatial-Temporal cross-validation\n",
    "- SHAP-based interpretability analysis\n",
    "- Performance analysis by temporal and spatial characteristics\n",
    "- Comprehensive model comparison and deployment recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c5efaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:57.268065Z",
     "iopub.status.busy": "2025-12-03T22:08:57.267704Z",
     "iopub.status.idle": "2025-12-03T22:08:57.333896Z",
     "shell.execute_reply": "2025-12-03T22:08:57.332985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 1: DATA LOADING\n",
      "======================================================================\n",
      "\n",
      "✓ Train data loaded from: ./data/processed_train_phase1.csv\n",
      "  Shape: (77013, 15) (Samples: 77013, Features: 15)\n",
      "\n",
      "✓ Test data loaded from: ./data/processed_test_phase1.csv\n",
      "  Shape: (19254, 15) (Samples: 19254, Features: 15)\n",
      "\n",
      "--- Train Dataset Head ---\n",
      "   passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "0              5.0       1.270718         -0.981244        -0.899854   \n",
      "1              1.0      -0.016575          0.432948         0.323212   \n",
      "2              0.0      -0.585635         -0.015630         0.819417   \n",
      "3              0.0      -0.381215          0.190997        -0.002930   \n",
      "4              5.0       2.988950         -0.856830        -0.506725   \n",
      "\n",
      "   dropoff_longitude  dropoff_latitude  fare_amount  tip_amount      hour  \\\n",
      "0          -0.642984         -2.451618     0.785714   -0.547945 -1.444444   \n",
      "1          -0.369988          0.045149    -0.071429   -0.091324 -1.444444   \n",
      "2           0.111439          1.006468    -0.785714   -0.091324 -1.444444   \n",
      "3           0.126179          0.314141    -0.500000    0.000000 -1.444444   \n",
      "4           1.095224         -3.102854     2.071429    0.547945 -1.444444   \n",
      "\n",
      "   day  month  weekday  is_weekend  is_rush_hour  total_amount  \n",
      "0  0.0    0.0      0.0         0.0           0.0          15.8  \n",
      "1  0.0    0.0      0.0         0.0           0.0          10.8  \n",
      "2  0.0    0.0      0.0         0.0           0.0           5.8  \n",
      "3  0.0    0.0      0.0         0.0           0.0           8.0  \n",
      "4  0.0    0.0      0.0         0.0           0.0          27.2  \n",
      "\n",
      "--- Train Dataset Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77013 entries, 0 to 77012\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   passenger_count    77013 non-null  float64\n",
      " 1   trip_distance      77013 non-null  float64\n",
      " 2   pickup_longitude   77013 non-null  float64\n",
      " 3   pickup_latitude    77013 non-null  float64\n",
      " 4   dropoff_longitude  77013 non-null  float64\n",
      " 5   dropoff_latitude   77013 non-null  float64\n",
      " 6   fare_amount        77013 non-null  float64\n",
      " 7   tip_amount         77013 non-null  float64\n",
      " 8   hour               77013 non-null  float64\n",
      " 9   day                77013 non-null  float64\n",
      " 10  month              77013 non-null  float64\n",
      " 11  weekday            77013 non-null  float64\n",
      " 12  is_weekend         77013 non-null  float64\n",
      " 13  is_rush_hour       77013 non-null  float64\n",
      " 14  total_amount       77013 non-null  float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 8.8 MB\n",
      "None\n",
      "\n",
      "--- Train Dataset Statistics ---\n",
      "       passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "count     77013.000000   77013.000000      77013.000000     77013.000000   \n",
      "mean          0.659304       0.581105          0.241042        -0.095839   \n",
      "std           1.361667       1.755402          1.670288         0.894137   \n",
      "min          -1.000000      -0.883978       -270.891250        -8.264084   \n",
      "25%           0.000000      -0.337017         -0.430447        -0.538154   \n",
      "50%           0.000000       0.000000          0.000000         0.000000   \n",
      "75%           1.000000       0.662983          0.569553         0.461846   \n",
      "max           5.000000      54.309392         11.562990         9.781196   \n",
      "\n",
      "       dropoff_longitude  dropoff_latitude   fare_amount    tip_amount  \\\n",
      "count       77013.000000      77013.000000  77013.000000  77013.000000   \n",
      "mean            0.210473         -0.084105      0.415527      0.177294   \n",
      "std             1.232281          0.954477      1.286519      0.929115   \n",
      "min            -7.987915         -6.547877     -1.284286     -0.547945   \n",
      "25%            -0.442217         -0.543056     -0.357143     -0.547945   \n",
      "50%             0.000000          0.000000      0.000000      0.000000   \n",
      "75%             0.557783          0.456944      0.642857      0.452055   \n",
      "max            13.423647          7.753839      8.428571     22.739726   \n",
      "\n",
      "               hour      day    month  weekday  is_weekend  is_rush_hour  \\\n",
      "count  77013.000000  77013.0  77013.0  77013.0     77013.0  77013.000000   \n",
      "mean      -0.068173      0.0      0.0      0.0         0.0      0.452807   \n",
      "std        0.609259      0.0      0.0      0.0         0.0      0.497771   \n",
      "min       -1.444444      0.0      0.0      0.0         0.0      0.000000   \n",
      "25%       -0.555556      0.0      0.0      0.0         0.0      0.000000   \n",
      "50%        0.000000      0.0      0.0      0.0         0.0      0.000000   \n",
      "75%        0.444444      0.0      0.0      0.0         0.0      1.000000   \n",
      "max        0.777778      0.0      0.0      0.0         0.0      1.000000   \n",
      "\n",
      "       total_amount  \n",
      "count  77013.000000  \n",
      "mean      14.892198  \n",
      "std       10.995076  \n",
      "min        4.800000  \n",
      "25%        8.300000  \n",
      "50%       11.400000  \n",
      "75%       16.550000  \n",
      "max       69.600000  \n",
      "\n",
      "✓ Train data loaded from: ./data/processed_train_phase1.csv\n",
      "  Shape: (77013, 15) (Samples: 77013, Features: 15)\n",
      "\n",
      "✓ Test data loaded from: ./data/processed_test_phase1.csv\n",
      "  Shape: (19254, 15) (Samples: 19254, Features: 15)\n",
      "\n",
      "--- Train Dataset Head ---\n",
      "   passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "0              5.0       1.270718         -0.981244        -0.899854   \n",
      "1              1.0      -0.016575          0.432948         0.323212   \n",
      "2              0.0      -0.585635         -0.015630         0.819417   \n",
      "3              0.0      -0.381215          0.190997        -0.002930   \n",
      "4              5.0       2.988950         -0.856830        -0.506725   \n",
      "\n",
      "   dropoff_longitude  dropoff_latitude  fare_amount  tip_amount      hour  \\\n",
      "0          -0.642984         -2.451618     0.785714   -0.547945 -1.444444   \n",
      "1          -0.369988          0.045149    -0.071429   -0.091324 -1.444444   \n",
      "2           0.111439          1.006468    -0.785714   -0.091324 -1.444444   \n",
      "3           0.126179          0.314141    -0.500000    0.000000 -1.444444   \n",
      "4           1.095224         -3.102854     2.071429    0.547945 -1.444444   \n",
      "\n",
      "   day  month  weekday  is_weekend  is_rush_hour  total_amount  \n",
      "0  0.0    0.0      0.0         0.0           0.0          15.8  \n",
      "1  0.0    0.0      0.0         0.0           0.0          10.8  \n",
      "2  0.0    0.0      0.0         0.0           0.0           5.8  \n",
      "3  0.0    0.0      0.0         0.0           0.0           8.0  \n",
      "4  0.0    0.0      0.0         0.0           0.0          27.2  \n",
      "\n",
      "--- Train Dataset Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77013 entries, 0 to 77012\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   passenger_count    77013 non-null  float64\n",
      " 1   trip_distance      77013 non-null  float64\n",
      " 2   pickup_longitude   77013 non-null  float64\n",
      " 3   pickup_latitude    77013 non-null  float64\n",
      " 4   dropoff_longitude  77013 non-null  float64\n",
      " 5   dropoff_latitude   77013 non-null  float64\n",
      " 6   fare_amount        77013 non-null  float64\n",
      " 7   tip_amount         77013 non-null  float64\n",
      " 8   hour               77013 non-null  float64\n",
      " 9   day                77013 non-null  float64\n",
      " 10  month              77013 non-null  float64\n",
      " 11  weekday            77013 non-null  float64\n",
      " 12  is_weekend         77013 non-null  float64\n",
      " 13  is_rush_hour       77013 non-null  float64\n",
      " 14  total_amount       77013 non-null  float64\n",
      "dtypes: float64(15)\n",
      "memory usage: 8.8 MB\n",
      "None\n",
      "\n",
      "--- Train Dataset Statistics ---\n",
      "       passenger_count  trip_distance  pickup_longitude  pickup_latitude  \\\n",
      "count     77013.000000   77013.000000      77013.000000     77013.000000   \n",
      "mean          0.659304       0.581105          0.241042        -0.095839   \n",
      "std           1.361667       1.755402          1.670288         0.894137   \n",
      "min          -1.000000      -0.883978       -270.891250        -8.264084   \n",
      "25%           0.000000      -0.337017         -0.430447        -0.538154   \n",
      "50%           0.000000       0.000000          0.000000         0.000000   \n",
      "75%           1.000000       0.662983          0.569553         0.461846   \n",
      "max           5.000000      54.309392         11.562990         9.781196   \n",
      "\n",
      "       dropoff_longitude  dropoff_latitude   fare_amount    tip_amount  \\\n",
      "count       77013.000000      77013.000000  77013.000000  77013.000000   \n",
      "mean            0.210473         -0.084105      0.415527      0.177294   \n",
      "std             1.232281          0.954477      1.286519      0.929115   \n",
      "min            -7.987915         -6.547877     -1.284286     -0.547945   \n",
      "25%            -0.442217         -0.543056     -0.357143     -0.547945   \n",
      "50%             0.000000          0.000000      0.000000      0.000000   \n",
      "75%             0.557783          0.456944      0.642857      0.452055   \n",
      "max            13.423647          7.753839      8.428571     22.739726   \n",
      "\n",
      "               hour      day    month  weekday  is_weekend  is_rush_hour  \\\n",
      "count  77013.000000  77013.0  77013.0  77013.0     77013.0  77013.000000   \n",
      "mean      -0.068173      0.0      0.0      0.0         0.0      0.452807   \n",
      "std        0.609259      0.0      0.0      0.0         0.0      0.497771   \n",
      "min       -1.444444      0.0      0.0      0.0         0.0      0.000000   \n",
      "25%       -0.555556      0.0      0.0      0.0         0.0      0.000000   \n",
      "50%        0.000000      0.0      0.0      0.0         0.0      0.000000   \n",
      "75%        0.444444      0.0      0.0      0.0         0.0      1.000000   \n",
      "max        0.777778      0.0      0.0      0.0         0.0      1.000000   \n",
      "\n",
      "       total_amount  \n",
      "count  77013.000000  \n",
      "mean      14.892198  \n",
      "std       10.995076  \n",
      "min        4.800000  \n",
      "25%        8.300000  \n",
      "50%       11.400000  \n",
      "75%       16.550000  \n",
      "max       69.600000  \n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 1: DATA LOADING ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 1: DATA LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the separate train and test datasets\n",
    "train_data_path = './data/processed_train_phase1.csv'\n",
    "test_data_path = './data/processed_test_phase1.csv'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    print(f\"\\n✓ Train data loaded from: {train_data_path}\")\n",
    "    print(f\"  Shape: {train_df.shape} (Samples: {train_df.shape[0]}, Features: {train_df.shape[1]})\")\n",
    "    print(f\"\\n✓ Test data loaded from: {test_data_path}\")\n",
    "    print(f\"  Shape: {test_df.shape} (Samples: {test_df.shape[0]}, Features: {test_df.shape[1]})\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\\n--- Train Dataset Head ---\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Train Dataset Info ---\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n--- Train Dataset Statistics ---\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f1ae6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:57.336316Z",
     "iopub.status.busy": "2025-12-03T22:08:57.336097Z",
     "iopub.status.idle": "2025-12-03T22:08:57.343699Z",
     "shell.execute_reply": "2025-12-03T22:08:57.342975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 2: FEATURE & TARGET SETUP\n",
      "======================================================================\n",
      "\n",
      "✓ Target variable: total_amount\n",
      "✓ Excluded columns: ['total_amount', 'fare_amount', 'tip_amount']\n",
      "✓ Number of features: 12\n",
      "\n",
      "Feature columns:\n",
      "  1. passenger_count\n",
      "  2. trip_distance\n",
      "  3. pickup_longitude\n",
      "  4. pickup_latitude\n",
      "  5. dropoff_longitude\n",
      "  6. dropoff_latitude\n",
      "  7. hour\n",
      "  8. day\n",
      "  9. month\n",
      "  10. weekday\n",
      "  11. is_weekend\n",
      "  12. is_rush_hour\n",
      "\n",
      "--- Data Shapes ---\n",
      "X_train shape: (77013, 12)\n",
      "y_train shape: (77013,)\n",
      "X_test shape: (19254, 12)\n",
      "y_test shape: (19254,)\n",
      "\n",
      "--- Missing Values ---\n",
      "Missing values in X_train: 0\n",
      "Missing values in X_test: 0\n",
      "\n",
      "✓ Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 2: DATA PREPARATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 2: FEATURE & TARGET SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define target variable\n",
    "target_col = 'total_amount'\n",
    "print(f\"\\n✓ Target variable: {target_col}\")\n",
    "\n",
    "# Define features - exclude target and its components\n",
    "exclude_cols = [target_col, 'fare_amount', 'tip_amount']\n",
    "feature_cols = [col for col in train_df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"✓ Excluded columns: {exclude_cols}\")\n",
    "print(f\"✓ Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# Create train/test splits\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"\\n--- Data Shapes ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "missing_train = X_train.isnull().sum().sum()\n",
    "missing_test = X_test.isnull().sum().sum()\n",
    "print(f\"Missing values in X_train: {missing_train}\")\n",
    "print(f\"Missing values in X_test: {missing_test}\")\n",
    "\n",
    "print(\"\\n✓ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61131bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:57.345478Z",
     "iopub.status.busy": "2025-12-03T22:08:57.345273Z",
     "iopub.status.idle": "2025-12-03T22:08:57.434443Z",
     "shell.execute_reply": "2025-12-03T22:08:57.433534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 3: TRAINING 6 REGRESSION MODELS\n",
      "======================================================================\n",
      "\n",
      "--- Model 1: XGBoost Regressor ---\n",
      "Training XGBoost...\n",
      "✓ XGBoost Training Complete\n",
      "  RMSE: 2.7744\n",
      "  MAE:  1.6614\n",
      "  R²:   0.9326\n",
      "✓ XGBoost Training Complete\n",
      "  RMSE: 2.7744\n",
      "  MAE:  1.6614\n",
      "  R²:   0.9326\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 3: MODEL TRAINING ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 3: TRAINING 6 REGRESSION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== MODEL 1: XGBoost Regressor ====================\n",
    "print(\"\\n--- Model 1: XGBoost Regressor ---\")\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_reg.predict(X_test)\n",
    "\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"✓ XGBoost Training Complete\")\n",
    "print(f\"  RMSE: {rmse_xgb:.4f}\")\n",
    "print(f\"  MAE:  {mae_xgb:.4f}\")\n",
    "print(f\"  R²:   {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ffda60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:57.436224Z",
     "iopub.status.busy": "2025-12-03T22:08:57.436021Z",
     "iopub.status.idle": "2025-12-03T22:08:57.448513Z",
     "shell.execute_reply": "2025-12-03T22:08:57.447585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model 2: Random Forest Regressor ---\n",
      "Training Random Forest...\n",
      "✓ Random Forest Training Complete\n",
      "  RMSE: 2.8280\n",
      "  MAE:  1.7217\n",
      "  R²:   0.9300\n",
      "\n",
      "Top 10 Most Important Features (Random Forest):\n",
      "              feature  importance\n",
      "1       trip_distance    0.939264\n",
      "6                hour    0.013637\n",
      "5    dropoff_latitude    0.013274\n",
      "4   dropoff_longitude    0.012480\n",
      "2    pickup_longitude    0.009610\n",
      "3     pickup_latitude    0.008865\n",
      "0     passenger_count    0.001673\n",
      "11       is_rush_hour    0.001196\n",
      "7                 day    0.000000\n",
      "8               month    0.000000\n",
      "✓ Random Forest Training Complete\n",
      "  RMSE: 2.8280\n",
      "  MAE:  1.7217\n",
      "  R²:   0.9300\n",
      "\n",
      "Top 10 Most Important Features (Random Forest):\n",
      "              feature  importance\n",
      "1       trip_distance    0.939264\n",
      "6                hour    0.013637\n",
      "5    dropoff_latitude    0.013274\n",
      "4   dropoff_longitude    0.012480\n",
      "2    pickup_longitude    0.009610\n",
      "3     pickup_latitude    0.008865\n",
      "0     passenger_count    0.001673\n",
      "11       is_rush_hour    0.001196\n",
      "7                 day    0.000000\n",
      "8               month    0.000000\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL 2: Random Forest Regressor ====================\n",
    "print(\"\\n--- Model 2: Random Forest Regressor ---\")\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"✓ Random Forest Training Complete\")\n",
    "print(f\"  RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"  MAE:  {mae_rf:.4f}\")\n",
    "print(f\"  R²:   {r2_rf:.4f}\")\n",
    "\n",
    "# Extract feature importance for RF\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d747682c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T22:08:57.450539Z",
     "iopub.status.busy": "2025-12-03T22:08:57.450332Z",
     "iopub.status.idle": "2025-12-03T22:08:57.456159Z",
     "shell.execute_reply": "2025-12-03T22:08:57.455385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model 3: Gradient Boosting Regressor ---\n",
      "Training Gradient Boosting...\n",
      "✓ Gradient Boosting Training Complete\n",
      "  RMSE: 2.6636\n",
      "  MAE:  1.6559\n",
      "  R²:   0.9379\n",
      "\n",
      "Top 10 Most Important Features (Gradient Boosting):\n",
      "              feature  importance\n",
      "1       trip_distance    0.962251\n",
      "6                hour    0.011883\n",
      "4   dropoff_longitude    0.008182\n",
      "5    dropoff_latitude    0.008131\n",
      "2    pickup_longitude    0.004503\n",
      "3     pickup_latitude    0.003943\n",
      "11       is_rush_hour    0.000828\n",
      "0     passenger_count    0.000279\n",
      "7                 day    0.000000\n",
      "8               month    0.000000\n",
      "✓ Gradient Boosting Training Complete\n",
      "  RMSE: 2.6636\n",
      "  MAE:  1.6559\n",
      "  R²:   0.9379\n",
      "\n",
      "Top 10 Most Important Features (Gradient Boosting):\n",
      "              feature  importance\n",
      "1       trip_distance    0.962251\n",
      "6                hour    0.011883\n",
      "4   dropoff_longitude    0.008182\n",
      "5    dropoff_latitude    0.008131\n",
      "2    pickup_longitude    0.004503\n",
      "3     pickup_latitude    0.003943\n",
      "11       is_rush_hour    0.000828\n",
      "0     passenger_count    0.000279\n",
      "7                 day    0.000000\n",
      "8               month    0.000000\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL 3: Gradient Boosting Regressor ====================\n",
    "print(\"\\n--- Model 3: Gradient Boosting Regressor ---\")\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=10,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_reg.fit(X_train, y_train)\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"✓ Gradient Boosting Training Complete\")\n",
    "print(f\"  RMSE: {rmse_gb:.4f}\")\n",
    "print(f\"  MAE:  {mae_gb:.4f}\")\n",
    "print(f\"  R²:   {r2_gb:.4f}\")\n",
    "\n",
    "# Extract feature importance for GB\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': gb_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Gradient Boosting):\")\n",
    "print(feature_importance_gb.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe522965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Models 4-6: Linear Regression, Ridge, and Lasso ---\n",
      "Standardizing features for regularized models...\n",
      "✓ Features standardized\n",
      "\n",
      "Training Linear Regression (Baseline)...\n",
      "✓ Linear Regression Training Complete\n",
      "  RMSE: 3.7173\n",
      "  MAE:  2.3575\n",
      "  R²:   0.8790\n",
      "\n",
      "Training Ridge Regression (L2)...\n",
      "✓ Ridge Regression Training Complete\n",
      "  RMSE: 3.7173\n",
      "  MAE:  2.3575\n",
      "  R²:   0.8790\n",
      "\n",
      "Training Lasso Regression (L1)...\n",
      "✓ Lasso Regression Training Complete\n",
      "  RMSE: 3.7143\n",
      "  MAE:  2.3554\n",
      "  R²:   0.8792\n",
      "\n",
      "======================================================================\n",
      "✓ ALL 6 MODELS TRAINED SUCCESSFULLY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODELS 4-6: Linear Regression & Regularized Models ====================\n",
    "print(\"\\n--- Models 4-6: Linear Regression, Ridge, and Lasso ---\")\n",
    "\n",
    "# Standardize features for regularized models\n",
    "print(\"Standardizing features for regularized models...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"✓ Features standardized\")\n",
    "\n",
    "# MODEL 4: Linear Regression (Baseline)\n",
    "print(\"\\nTraining Linear Regression (Baseline)...\")\n",
    "lr_reg = LinearRegression()\n",
    "lr_reg.fit(X_train, y_train)\n",
    "y_pred_lr = lr_reg.predict(X_test)\n",
    "\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"✓ Linear Regression Training Complete\")\n",
    "print(f\"  RMSE: {rmse_lr:.4f}\")\n",
    "print(f\"  MAE:  {mae_lr:.4f}\")\n",
    "print(f\"  R²:   {r2_lr:.4f}\")\n",
    "\n",
    "# MODEL 5: Ridge Regression (L2 Regularization)\n",
    "print(\"\\nTraining Ridge Regression (L2)...\")\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_reg.predict(X_test_scaled)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"✓ Ridge Regression Training Complete\")\n",
    "print(f\"  RMSE: {rmse_ridge:.4f}\")\n",
    "print(f\"  MAE:  {mae_ridge:.4f}\")\n",
    "print(f\"  R²:   {r2_ridge:.4f}\")\n",
    "\n",
    "# MODEL 6: Lasso Regression (L1 Regularization)\n",
    "print(\"\\nTraining Lasso Regression (L1)...\")\n",
    "lasso_reg = Lasso(alpha=0.01)\n",
    "lasso_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lasso = lasso_reg.predict(X_test_scaled)\n",
    "\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"✓ Lasso Regression Training Complete\")\n",
    "print(f\"  RMSE: {rmse_lasso:.4f}\")\n",
    "print(f\"  MAE:  {mae_lasso:.4f}\")\n",
    "print(f\"  R²:   {r2_lasso:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL 6 MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd43d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 4: ADVANCED EVALUATION METRICS\n",
      "======================================================================\n",
      "\n",
      "=== Advanced Metrics Comparison ===\n",
      "\n",
      "                       RMSE       MAE        R2      MAPE  Quantile_Loss_0.25  \\\n",
      "XGBoost            2.774382  1.661356  0.932612  0.115553            0.935651   \n",
      "Random Forest      2.828006  1.721698  0.929982  0.120531            1.027025   \n",
      "Gradient Boosting  2.663600  1.655924  0.937886  0.115174            0.937920   \n",
      "Linear Regression  3.717276  2.357502  0.879023  0.191585            1.546342   \n",
      "Ridge Regression   3.717264  2.357518  0.879024  0.191589            1.546342   \n",
      "Lasso Regression   3.714251  2.355394  0.879220  0.191462            1.543073   \n",
      "\n",
      "                   Quantile_Loss_0.5  Quantile_Loss_0.75  \n",
      "XGBoost                     0.830678            0.725706  \n",
      "Random Forest               0.860849            0.694673  \n",
      "Gradient Boosting           0.827962            0.718004  \n",
      "Linear Regression           1.178751            0.811161  \n",
      "Ridge Regression            1.178759            0.811176  \n",
      "Lasso Regression            1.177697            0.812321  \n",
      "\n",
      "✓ Advanced metrics saved to 'phase2_advanced_metrics.json'\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 4: ADVANCED METRICS (MAPE & QUANTILE LOSS) ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 4: ADVANCED EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define quantile (pinball) loss\n",
    "def quantile_loss(y_true, y_pred, q=0.5):\n",
    "    resid = y_true - y_pred\n",
    "    return np.mean(np.maximum(q * resid, (q - 1) * resid))\n",
    "\n",
    "# Compute metrics for each model\n",
    "metrics_dict = {}\n",
    "\n",
    "metrics_dict['XGBoost'] = {\n",
    "    'RMSE': float(rmse_xgb),\n",
    "    'MAE': float(mae_xgb),\n",
    "    'R2': float(r2_xgb),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_xgb)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_xgb, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_xgb, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_xgb, 0.75))\n",
    "}\n",
    "\n",
    "metrics_dict['Random Forest'] = {\n",
    "    'RMSE': float(rmse_rf),\n",
    "    'MAE': float(mae_rf),\n",
    "    'R2': float(r2_rf),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_rf)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_rf, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_rf, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_rf, 0.75))\n",
    "}\n",
    "\n",
    "metrics_dict['Gradient Boosting'] = {\n",
    "    'RMSE': float(rmse_gb),\n",
    "    'MAE': float(mae_gb),\n",
    "    'R2': float(r2_gb),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_gb)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_gb, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_gb, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_gb, 0.75))\n",
    "}\n",
    "\n",
    "metrics_dict['Linear Regression'] = {\n",
    "    'RMSE': float(rmse_lr),\n",
    "    'MAE': float(mae_lr),\n",
    "    'R2': float(r2_lr),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_lr)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_lr, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_lr, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_lr, 0.75))\n",
    "}\n",
    "\n",
    "metrics_dict['Ridge Regression'] = {\n",
    "    'RMSE': float(rmse_ridge),\n",
    "    'MAE': float(mae_ridge),\n",
    "    'R2': float(r2_ridge),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_ridge)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_ridge, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_ridge, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_ridge, 0.75))\n",
    "}\n",
    "\n",
    "metrics_dict['Lasso Regression'] = {\n",
    "    'RMSE': float(rmse_lasso),\n",
    "    'MAE': float(mae_lasso),\n",
    "    'R2': float(r2_lasso),\n",
    "    'MAPE': float(mean_absolute_percentage_error(y_test, y_pred_lasso)),\n",
    "    'Quantile_Loss_0.25': float(quantile_loss(y_test.values, y_pred_lasso, 0.25)),\n",
    "    'Quantile_Loss_0.5': float(quantile_loss(y_test.values, y_pred_lasso, 0.5)),\n",
    "    'Quantile_Loss_0.75': float(quantile_loss(y_test.values, y_pred_lasso, 0.75))\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "metrics_df = pd.DataFrame(metrics_dict).T\n",
    "print(\"\\n=== Advanced Metrics Comparison ===\\n\")\n",
    "print(metrics_df.round(6))\n",
    "\n",
    "# Save metrics to a JSON file\n",
    "with open('phase2_advanced_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_dict, f, indent=4)\n",
    "\n",
    "print(\"\\n✓ Advanced metrics saved to 'phase2_advanced_metrics.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba3dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 5: SPATIAL-TEMPORAL CROSS-VALIDATION (5 folds)\n",
      "======================================================================\n",
      "Running TimeSeriesSplit with 5 folds on 96267 samples...\n",
      "\n",
      "Fold 1: train=16047 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.0506\n",
      "  Random Forest RMSE: 3.1526\n",
      "  Gradient Boosting RMSE: 3.1033\n",
      "  Linear Regression RMSE: 4.6122\n",
      "  Ridge RMSE: 4.6122\n",
      "  Lasso RMSE: 4.5830\n",
      "\n",
      "Fold 2: train=32091 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.0506\n",
      "  Random Forest RMSE: 3.1526\n",
      "  Gradient Boosting RMSE: 3.1033\n",
      "  Linear Regression RMSE: 4.6122\n",
      "  Ridge RMSE: 4.6122\n",
      "  Lasso RMSE: 4.5830\n",
      "\n",
      "Fold 2: train=32091 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.0207\n",
      "  Random Forest RMSE: 3.0379\n",
      "  Gradient Boosting RMSE: 3.0414\n",
      "  Linear Regression RMSE: 3.8714\n",
      "  Ridge RMSE: 3.8715\n",
      "  Lasso RMSE: 3.8719\n",
      "\n",
      "Fold 3: train=48135 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.0207\n",
      "  Random Forest RMSE: 3.0379\n",
      "  Gradient Boosting RMSE: 3.0414\n",
      "  Linear Regression RMSE: 3.8714\n",
      "  Ridge RMSE: 3.8715\n",
      "  Lasso RMSE: 3.8719\n",
      "\n",
      "Fold 3: train=48135 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.3402\n",
      "  Random Forest RMSE: 3.4687\n",
      "  Gradient Boosting RMSE: 3.3184\n",
      "  Linear Regression RMSE: 4.1078\n",
      "  Ridge RMSE: 4.1078\n",
      "  Lasso RMSE: 4.1064\n",
      "\n",
      "Fold 4: train=64179 samples, test=16044 samples\n",
      "  XGBoost RMSE: 3.3402\n",
      "  Random Forest RMSE: 3.4687\n",
      "  Gradient Boosting RMSE: 3.3184\n",
      "  Linear Regression RMSE: 4.1078\n",
      "  Ridge RMSE: 4.1078\n",
      "  Lasso RMSE: 4.1064\n",
      "\n",
      "Fold 4: train=64179 samples, test=16044 samples\n",
      "  XGBoost RMSE: 2.8750\n",
      "  Random Forest RMSE: 3.0113\n",
      "  Gradient Boosting RMSE: 2.9134\n",
      "  Linear Regression RMSE: 3.2737\n",
      "  Ridge RMSE: 3.2737\n",
      "  Lasso RMSE: 3.2693\n",
      "\n",
      "Fold 5: train=80223 samples, test=16044 samples\n",
      "  XGBoost RMSE: 2.8750\n",
      "  Random Forest RMSE: 3.0113\n",
      "  Gradient Boosting RMSE: 2.9134\n",
      "  Linear Regression RMSE: 3.2737\n",
      "  Ridge RMSE: 3.2737\n",
      "  Lasso RMSE: 3.2693\n",
      "\n",
      "Fold 5: train=80223 samples, test=16044 samples\n",
      "  XGBoost RMSE: 2.7892\n",
      "  Random Forest RMSE: 2.7593\n",
      "  Gradient Boosting RMSE: 2.6659\n",
      "  Linear Regression RMSE: 3.7515\n",
      "  Ridge RMSE: 3.7515\n",
      "  Lasso RMSE: 3.7484\n",
      "\n",
      "\n",
      "=== Cross-Validation Summary ===\n",
      "\n",
      "                  mean_rmse  std_rmse  \\\n",
      "XGBoost            3.015124  0.210727   \n",
      "Random Forest      3.085935  0.257643   \n",
      "Gradient Boosting  3.008469  0.241105   \n",
      "Linear Regression  3.923317   0.49062   \n",
      "Ridge              3.923336   0.49062   \n",
      "Lasso              3.915793  0.482015   \n",
      "\n",
      "                                                           fold_rmse  \n",
      "XGBoost            [3.0506036108501067, 3.020674764471124, 3.3401...  \n",
      "Random Forest      [3.1525789825598856, 3.03785595859822, 3.46867...  \n",
      "Gradient Boosting  [3.1033206329053202, 3.0413618244446017, 3.318...  \n",
      "Linear Regression  [4.61221987145036, 3.8713728603322197, 4.10776...  \n",
      "Ridge              [4.612205877893987, 3.871451991211893, 4.10781...  \n",
      "Lasso              [4.582977376748517, 3.8719114566315023, 4.1063...  \n",
      "\n",
      "✓ Cross-validation results saved to 'phase2_cv_results.json'\n",
      "  XGBoost RMSE: 2.7892\n",
      "  Random Forest RMSE: 2.7593\n",
      "  Gradient Boosting RMSE: 2.6659\n",
      "  Linear Regression RMSE: 3.7515\n",
      "  Ridge RMSE: 3.7515\n",
      "  Lasso RMSE: 3.7484\n",
      "\n",
      "\n",
      "=== Cross-Validation Summary ===\n",
      "\n",
      "                  mean_rmse  std_rmse  \\\n",
      "XGBoost            3.015124  0.210727   \n",
      "Random Forest      3.085935  0.257643   \n",
      "Gradient Boosting  3.008469  0.241105   \n",
      "Linear Regression  3.923317   0.49062   \n",
      "Ridge              3.923336   0.49062   \n",
      "Lasso              3.915793  0.482015   \n",
      "\n",
      "                                                           fold_rmse  \n",
      "XGBoost            [3.0506036108501067, 3.020674764471124, 3.3401...  \n",
      "Random Forest      [3.1525789825598856, 3.03785595859822, 3.46867...  \n",
      "Gradient Boosting  [3.1033206329053202, 3.0413618244446017, 3.318...  \n",
      "Linear Regression  [4.61221987145036, 3.8713728603322197, 4.10776...  \n",
      "Ridge              [4.612205877893987, 3.871451991211893, 4.10781...  \n",
      "Lasso              [4.582977376748517, 3.8719114566315023, 4.1063...  \n",
      "\n",
      "✓ Cross-validation results saved to 'phase2_cv_results.json'\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 5: SPATIAL-TEMPORAL CROSS-VALIDATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 5: SPATIAL-TEMPORAL CROSS-VALIDATION (5 folds)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine train and test for CV (ensure data is time-ordered for TimeSeriesSplit)\n",
    "full_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "X_full = full_data[feature_cols]\n",
    "y_full = full_data[target_col]\n",
    "\n",
    "# TimeSeriesSplit\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "cv_results = {\n",
    "    'fold': [],\n",
    "    'xgb_rmse': [],\n",
    "    'rf_rmse': [],\n",
    "    'gb_rmse': [],\n",
    "    'lr_rmse': [],\n",
    "    'ridge_rmse': [],\n",
    "    'lasso_rmse': []\n",
    "}\n",
    "\n",
    "print(f\"Running TimeSeriesSplit with {n_splits} folds on {len(X_full)} samples...\\n\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_full), start=1):\n",
    "    print(f\"Fold {i}: train={len(train_idx)} samples, test={len(test_idx)} samples\")\n",
    "    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[test_idx]\n",
    "    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[test_idx]\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_cv = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, n_jobs=-1, verbosity=0)\n",
    "    xgb_cv.fit(X_tr, y_tr)\n",
    "    y_pred_xgb_cv = xgb_cv.predict(X_val)\n",
    "    rmse_xgb_cv = np.sqrt(mean_squared_error(y_val, y_pred_xgb_cv))\n",
    "\n",
    "    # Random Forest\n",
    "    rf_cv = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "    rf_cv.fit(X_tr, y_tr)\n",
    "    y_pred_rf_cv = rf_cv.predict(X_val)\n",
    "    rmse_rf_cv = np.sqrt(mean_squared_error(y_val, y_pred_rf_cv))\n",
    "\n",
    "    # Gradient Boosting\n",
    "    gb_cv = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "    gb_cv.fit(X_tr, y_tr)\n",
    "    y_pred_gb_cv = gb_cv.predict(X_val)\n",
    "    rmse_gb_cv = np.sqrt(mean_squared_error(y_val, y_pred_gb_cv))\n",
    "\n",
    "    # Linear Regression\n",
    "    lr_cv = LinearRegression()\n",
    "    lr_cv.fit(X_tr, y_tr)\n",
    "    y_pred_lr_cv = lr_cv.predict(X_val)\n",
    "    rmse_lr_cv = np.sqrt(mean_squared_error(y_val, y_pred_lr_cv))\n",
    "\n",
    "    # Ridge & Lasso require scaling\n",
    "    scaler_cv = StandardScaler()\n",
    "    X_tr_s = scaler_cv.fit_transform(X_tr)\n",
    "    X_val_s = scaler_cv.transform(X_val)\n",
    "\n",
    "    ridge_cv = Ridge(alpha=1.0)\n",
    "    ridge_cv.fit(X_tr_s, y_tr)\n",
    "    y_pred_ridge_cv = ridge_cv.predict(X_val_s)\n",
    "    rmse_ridge_cv = np.sqrt(mean_squared_error(y_val, y_pred_ridge_cv))\n",
    "\n",
    "    lasso_cv = Lasso(alpha=0.01)\n",
    "    lasso_cv.fit(X_tr_s, y_tr)\n",
    "    y_pred_lasso_cv = lasso_cv.predict(X_val_s)\n",
    "    rmse_lasso_cv = np.sqrt(mean_squared_error(y_val, y_pred_lasso_cv))\n",
    "\n",
    "    # Record results\n",
    "    cv_results['fold'].append(i)\n",
    "    cv_results['xgb_rmse'].append(float(rmse_xgb_cv))\n",
    "    cv_results['rf_rmse'].append(float(rmse_rf_cv))\n",
    "    cv_results['gb_rmse'].append(float(rmse_gb_cv))\n",
    "    cv_results['lr_rmse'].append(float(rmse_lr_cv))\n",
    "    cv_results['ridge_rmse'].append(float(rmse_ridge_cv))\n",
    "    cv_results['lasso_rmse'].append(float(rmse_lasso_cv))\n",
    "\n",
    "    print(f\"  XGBoost RMSE: {rmse_xgb_cv:.4f}\")\n",
    "    print(f\"  Random Forest RMSE: {rmse_rf_cv:.4f}\")\n",
    "    print(f\"  Gradient Boosting RMSE: {rmse_gb_cv:.4f}\")\n",
    "    print(f\"  Linear Regression RMSE: {rmse_lr_cv:.4f}\")\n",
    "    print(f\"  Ridge RMSE: {rmse_ridge_cv:.4f}\")\n",
    "    print(f\"  Lasso RMSE: {rmse_lasso_cv:.4f}\\n\")\n",
    "\n",
    "# Summarize CV results\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "summary = {}\n",
    "for model_label, col in [('XGBoost','xgb_rmse'), ('Random Forest','rf_rmse'), ('Gradient Boosting','gb_rmse'), ('Linear Regression','lr_rmse'), ('Ridge','ridge_rmse'), ('Lasso','lasso_rmse')]:\n",
    "    vals = cv_df[col]\n",
    "    summary[model_label] = {\n",
    "        'mean_rmse': float(vals.mean()),\n",
    "        'std_rmse': float(vals.std()),\n",
    "        'fold_rmse': list(map(float, vals.tolist()))\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Cross-Validation Summary ===\\n\")\n",
    "print(pd.DataFrame(summary).T.round(4))\n",
    "\n",
    "# Save CV results\n",
    "with open('phase2_cv_results.json', 'w') as f:\n",
    "    json.dump({'cv_summary': summary, 'cv_folds': cv_results}, f, indent=4)\n",
    "\n",
    "print(\"\\n✓ Cross-validation results saved to 'phase2_cv_results.json'\")\n",
    "\n",
    "# Store cv_df for downstream plotting\n",
    "cv_results_df = cv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3140f9",
   "metadata": {},
   "source": [
    "## Section 6: SHAP Analysis & Model Interpretability\n",
    "\n",
    "This section computes SHAP values for the primary model (XGBoost) and generates visualizations to explain feature contributions to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a730c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 6: SHAP ANALYSIS & MODEL INTERPRETABILITY\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omar8\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP library loaded.\n",
      "\n",
      "Computing SHAP values for XGBoost model (this may take some time)...\n",
      "SHAP values computed.\n",
      "Generating SHAP summary (bar) plot...\n",
      "SHAP values computed.\n",
      "Generating SHAP summary (bar) plot...\n",
      "Saved 'shap_summary_bar.png'\n",
      "Generating SHAP beeswarm plot...\n",
      "Saved 'shap_summary_bar.png'\n",
      "Generating SHAP beeswarm plot...\n",
      "Saved 'shap_summary_beeswarm.png'\n",
      "Generating SHAP dependence plots for top features...\n",
      "Saved 'shap_summary_beeswarm.png'\n",
      "Generating SHAP dependence plots for top features...\n",
      "Saved dependence plots:\n",
      "  - shap_dependence_trip_distance.png\n",
      "  - shap_dependence_hour.png\n",
      "  - shap_dependence_dropoff_longitude.png\n",
      "  - shap_dependence_dropoff_latitude.png\n",
      "\n",
      "SHAP analysis completed. Plots saved to working directory.\n",
      "Saved dependence plots:\n",
      "  - shap_dependence_trip_distance.png\n",
      "  - shap_dependence_hour.png\n",
      "  - shap_dependence_dropoff_longitude.png\n",
      "  - shap_dependence_dropoff_latitude.png\n",
      "\n",
      "SHAP analysis completed. Plots saved to working directory.\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 6: SHAP ANALYSIS & INTERPRETABILITY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 6: SHAP ANALYSIS & MODEL INTERPRETABILITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure SHAP is installed and importable\n",
    "import shap\n",
    "\n",
    "\n",
    "# Compute SHAP values for XGBoost model\n",
    "print(\"\\nComputing SHAP values for XGBoost model (this may take some time)...\")\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(xgb_reg)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "except Exception:\n",
    "    # Fallback to the newer API\n",
    "    explainer = shap.Explainer(xgb_reg, X_train)\n",
    "    shap_exp = explainer(X_test)\n",
    "    try:\n",
    "        shap_values = shap_exp.values\n",
    "    except Exception:\n",
    "        shap_values = shap_exp\n",
    "\n",
    "print(\"SHAP values computed.\")\n",
    "\n",
    "# Create and save SHAP visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "# 1) SHAP summary (bar)\n",
    "print(\"Generating SHAP summary (bar) plot...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "try:\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='bar', show=False)\n",
    "    plt.title('SHAP Summary - Feature Importance (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary_bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved 'shap_summary_bar.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create SHAP summary bar plot: {e}\")\n",
    "\n",
    "# 2) SHAP beeswarm (impact distribution)\n",
    "print(\"Generating SHAP beeswarm plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "try:\n",
    "    shap.summary_plot(shap_values, X_test, show=False)\n",
    "    plt.title('SHAP Summary - Feature Impact (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_summary_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved 'shap_summary_beeswarm.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create SHAP beeswarm plot: {e}\")\n",
    "\n",
    "# 3) SHAP dependence plots for top features\n",
    "print(\"Generating SHAP dependence plots for top features...\")\n",
    "try:\n",
    "    top_feats = feature_importance_gb.head(4)['feature'].values.tolist()\n",
    "except Exception:\n",
    "    # Fall back to RF importance if GB not available\n",
    "    top_feats = feature_importance_rf.head(4)['feature'].values.tolist()\n",
    "\n",
    "saved_dependence = []\n",
    "for feat in top_feats:\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        shap.dependence_plot(feat, shap_values, X_test, show=False)\n",
    "        fname = f'shap_dependence_{feat}.png'\n",
    "        plt.title(f'SHAP Dependence - {feat}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        saved_dependence.append(fname)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create dependence plot for {feat}: {e}\")\n",
    "\n",
    "if saved_dependence:\n",
    "    print(\"Saved dependence plots:\")\n",
    "    for p in saved_dependence:\n",
    "        print(f\"  - {p}\")\n",
    "else:\n",
    "    print(\"No dependence plots were saved.\")\n",
    "\n",
    "print(\"\\nSHAP analysis completed. Plots saved to working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a36e28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 7: PERFORMANCE BY TEMPORAL FEATURES\n",
      "======================================================================\n",
      "\n",
      "--- Performance by hour ---\n",
      "    group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  \\\n",
      "0  0.7778         2.7892        1.6508       0.9381        3.0147   \n",
      "1  0.8889         2.6505        1.5552       0.9378        2.7404   \n",
      "2  1.0000         2.6563        1.6309       0.9360        2.7573   \n",
      "3  1.1111         3.0194        1.8010       0.9238        2.9821   \n",
      "\n",
      "   rf_pred_MAE  rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  \\\n",
      "0       1.7373      0.9277        2.4346       1.5990      0.9529   \n",
      "1       1.6206      0.9335        2.6797       1.5722      0.9364   \n",
      "2       1.6826      0.9310        2.5029       1.6214      0.9431   \n",
      "3       1.8657      0.9257        2.8409       1.7811      0.9326   \n",
      "\n",
      "   lr_pred_RMSE  lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  \\\n",
      "0        3.4957       2.2743      0.9028           3.4957          2.2743   \n",
      "1        3.1280       2.1723      0.9133           3.1280          2.1723   \n",
      "2        4.0182       2.3121      0.8535           4.0181          2.3122   \n",
      "3        3.8695       2.5950      0.8749           3.8695          2.5950   \n",
      "\n",
      "   ridge_pred_R2  lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0         0.9028           3.4980          2.2740         0.9027    285  \n",
      "1         0.9133           3.1277          2.1714         0.9134   5767  \n",
      "2         0.8535           4.0143          2.3110         0.8537   7224  \n",
      "3         0.8749           3.8651          2.5904         0.8752   5978  \n",
      "Saved temporal_performance_by_hour.csv\n",
      "Saved plot temporal_rmse_by_hour.png\n",
      "\n",
      "--- Performance by day ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_day.csv\n",
      "Saved plot temporal_rmse_by_hour.png\n",
      "\n",
      "--- Performance by day ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_day.csv\n",
      "Saved plot temporal_rmse_by_day.png\n",
      "\n",
      "--- Performance by is_rush_hour ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_is_rush_hour.csv\n",
      "Saved plot temporal_rmse_by_day.png\n",
      "\n",
      "--- Performance by is_rush_hour ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_is_rush_hour.csv\n",
      "Saved plot temporal_rmse_by_is_rush_hour.png\n",
      "\n",
      "--- Performance by is_weekend ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_is_weekend.csv\n",
      "Saved plot temporal_rmse_by_is_rush_hour.png\n",
      "\n",
      "--- Performance by is_weekend ---\n",
      "   group  xgb_pred_RMSE  xgb_pred_MAE  xgb_pred_R2  rf_pred_RMSE  rf_pred_MAE  \\\n",
      "0    0.0         2.7744        1.6614       0.9326         2.828       1.7217   \n",
      "\n",
      "   rf_pred_R2  gb_pred_RMSE  gb_pred_MAE  gb_pred_R2  lr_pred_RMSE  \\\n",
      "0        0.93        2.6636       1.6559      0.9379        3.7173   \n",
      "\n",
      "   lr_pred_MAE  lr_pred_R2  ridge_pred_RMSE  ridge_pred_MAE  ridge_pred_R2  \\\n",
      "0       2.3575       0.879           3.7173          2.3575          0.879   \n",
      "\n",
      "   lasso_pred_RMSE  lasso_pred_MAE  lasso_pred_R2  count  \n",
      "0           3.7143          2.3554         0.8792  19254  \n",
      "Saved temporal_performance_by_is_weekend.csv\n",
      "Saved plot temporal_rmse_by_is_weekend.png\n",
      "\n",
      "✓ Temporal performance analysis complete.\n",
      "Saved plot temporal_rmse_by_is_weekend.png\n",
      "\n",
      "✓ Temporal performance analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 7: PERFORMANCE BY TEMPORAL FEATURES ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SECTION 7: PERFORMANCE BY TEMPORAL FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare test Analysis DF\n",
    "test_analysis = X_test.copy()\n",
    "if 'actual' not in test_analysis.columns:\n",
    "    test_analysis['actual'] = y_test.values\n",
    "# Add predictions from models (if present)\n",
    "model_preds = {\n",
    "    'xgb_pred': 'y_pred_xgb',\n",
    "    'rf_pred': 'y_pred_rf',\n",
    "    'gb_pred': 'y_pred_gb',\n",
    "    'lr_pred': 'y_pred_lr',\n",
    "    'ridge_pred': 'y_pred_ridge',\n",
    "    'lasso_pred': 'y_pred_lasso'\n",
    "}\n",
    "for col_name, var_name in model_preds.items():\n",
    "    if var_name in globals():\n",
    "        test_analysis[col_name] = globals()[var_name]\n",
    "\n",
    "# Helper to compute metrics\n",
    "def group_metrics(df, actual_col, pred_col):\n",
    "    return pd.Series({\n",
    "        'RMSE': np.sqrt(mean_squared_error(df[actual_col], df[pred_col])),\n",
    "        'MAE': mean_absolute_error(df[actual_col], df[pred_col]),\n",
    "        'R2': r2_score(df[actual_col], df[pred_col]),\n",
    "        'Count': len(df)\n",
    "    })\n",
    "\n",
    "# Choose temporal columns to evaluate\n",
    "temporal_cols = [c for c in ['hour','day','is_rush_hour','is_weekend'] if c in test_analysis.columns]\n",
    "\n",
    "for col in temporal_cols:\n",
    "    print(f\"\\n--- Performance by {col} ---\")\n",
    "    results = []\n",
    "    groups = test_analysis.groupby(col, observed=True)\n",
    "    for name, grp in groups:\n",
    "        row = {'group': name}\n",
    "        for model_col in ['xgb_pred','rf_pred','gb_pred','lr_pred','ridge_pred','lasso_pred']:\n",
    "            if model_col in grp.columns:\n",
    "                m = group_metrics(grp, 'actual', model_col)\n",
    "                row[f\"{model_col}_RMSE\"] = m['RMSE']\n",
    "                row[f\"{model_col}_MAE\"] = m['MAE']\n",
    "                row[f\"{model_col}_R2\"] = m['R2']\n",
    "        row['count'] = len(grp)\n",
    "        results.append(row)\n",
    "    res_df = pd.DataFrame(results).sort_values('group')\n",
    "    print(res_df.round(4))\n",
    "    csv_name = f'temporal_performance_by_{col}.csv'\n",
    "    res_df.to_csv(csv_name, index=False)\n",
    "    print(f\"Saved {csv_name}\")\n",
    "\n",
    "    # Plot RMSE heatmap/bar for available models\n",
    "    rmse_cols = [c for c in res_df.columns if c.endswith('_RMSE')]\n",
    "    if len(rmse_cols) > 0:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        # plot RMSE for first three models to keep plot readable\n",
    "        plot_cols = rmse_cols[:6]\n",
    "        res_plot = res_df.set_index('group')[plot_cols]\n",
    "        res_plot.plot(kind='bar', figsize=(12,6))\n",
    "        plt.title(f'RMSE by {col}')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        img_name = f'temporal_rmse_by_{col}.png'\n",
    "        plt.savefig(img_name, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved plot {img_name}\")\n",
    "\n",
    "print('\\n✓ Temporal performance analysis complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ed992df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 8: SPATIAL PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Analyzing performance by trip distance categories...\n",
      "Saved spatial_performance_by_distance.csv\n",
      "Saved plot spatial_rmse_by_distance.png\n",
      "\n",
      "Analyzing performance by passenger_count...\n",
      "Saved spatial_performance_by_passenger_count.csv\n",
      "Saved plot spatial_rmse_by_distance.png\n",
      "\n",
      "Analyzing performance by passenger_count...\n",
      "Saved spatial_performance_by_passenger_count.csv\n",
      "Saved plot spatial_rmse_by_passenger_count.png\n",
      "\n",
      "✓ Spatial performance analysis complete.\n",
      "Saved plot spatial_rmse_by_passenger_count.png\n",
      "\n",
      "✓ Spatial performance analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 8: SPATIAL PERFORMANCE ANALYSIS ====================\n",
    "print('\\n' + '='*70)\n",
    "print('SECTION 8: SPATIAL PERFORMANCE ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Ensure 'test_analysis' exists (created in Section 7) and the helper 'group_metrics' is available\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    group_metrics\n",
    "except NameError:\n",
    "    def group_metrics(df, actual_col, pred_col):\n",
    "        return pd.Series({\n",
    "            'RMSE': np.sqrt(mean_squared_error(df[actual_col], df[pred_col])) ,\n",
    "            'MAE': mean_absolute_error(df[actual_col], df[pred_col]),\n",
    "            'R2': r2_score(df[actual_col], df[pred_col]),\n",
    "            'Count': len(df)\n",
    "        })\n",
    "\n",
    "if 'test_analysis' not in globals():\n",
    "    test_analysis = X_test.copy()\n",
    "    test_analysis['actual'] = y_test.values\n",
    "    model_preds = {\n",
    "        'xgb_pred': 'y_pred_xgb',\n",
    "        'rf_pred': 'y_pred_rf',\n",
    "        'gb_pred': 'y_pred_gb',\n",
    "        'lr_pred': 'y_pred_lr',\n",
    "        'ridge_pred': 'y_pred_ridge',\n",
    "        'lasso_pred': 'y_pred_lasso'\n",
    "    }\n",
    "    for col_name, var_name in model_preds.items():\n",
    "        if var_name in globals():\n",
    "            test_analysis[col_name] = globals()[var_name]\n",
    "\n",
    "# 1) Performance by trip_distance categories\n",
    "if 'trip_distance' in test_analysis.columns:\n",
    "    print('\\nAnalyzing performance by trip distance categories...')\n",
    "    # Define bins; fall back to quantiles if necessary\n",
    "    try:\n",
    "        bins = [0, 1, 3, 10, np.inf]\n",
    "        labels = ['Very Short', 'Short', 'Medium', 'Long']\n",
    "        test_analysis['distance_cat'] = pd.cut(test_analysis['trip_distance'], bins=bins, labels=labels, include_lowest=True)\n",
    "    except Exception:\n",
    "        test_analysis['distance_cat'] = pd.qcut(test_analysis['trip_distance'], 4, labels=['Very Short','Short','Medium','Long'])\n",
    "\n",
    "    results = []\n",
    "    groups = test_analysis.groupby('distance_cat', observed=True)\n",
    "    for name, grp in groups:\n",
    "        row = {'group': str(name)}\n",
    "        for model_col in ['xgb_pred','rf_pred','gb_pred','lr_pred','ridge_pred','lasso_pred']:\n",
    "            if model_col in grp.columns:\n",
    "                m = group_metrics(grp, 'actual', model_col)\n",
    "                row[f'{model_col}_RMSE'] = m['RMSE']\n",
    "                row[f'{model_col}_MAE'] = m['MAE']\n",
    "                row[f'{model_col}_R2'] = m['R2']\n",
    "        row['count'] = len(grp)\n",
    "        results.append(row)\n",
    "    res_df = pd.DataFrame(results).sort_values('group')\n",
    "    csv_name = 'spatial_performance_by_distance.csv'\n",
    "    res_df.to_csv(csv_name, index=False)\n",
    "    print(f'Saved {csv_name}')\n",
    "\n",
    "    # Plot RMSE by distance category\n",
    "    rmse_cols = [c for c in res_df.columns if c.endswith('_RMSE')]\n",
    "    if len(rmse_cols) > 0:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_df = res_df.set_index('group')[rmse_cols]\n",
    "        plot_df.plot(kind='bar', figsize=(10,6))\n",
    "        plt.title('RMSE by Trip Distance Category')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('Distance Category')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        img_name = 'spatial_rmse_by_distance.png'\n",
    "        plt.savefig(img_name, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'Saved plot {img_name}')\n",
    "else:\n",
    "    print('trip_distance column not found in test data; skipping distance-based analysis.')\n",
    "\n",
    "# 2) Performance by passenger_count\n",
    "if 'passenger_count' in test_analysis.columns:\n",
    "    print('\\nAnalyzing performance by passenger_count...')\n",
    "    test_analysis['passenger_count_group'] = test_analysis['passenger_count'].clip(lower=0).astype(int)\n",
    "    results = []\n",
    "    groups = test_analysis.groupby('passenger_count_group', observed=True)\n",
    "    for name, grp in groups:\n",
    "        row = {'group': int(name)}\n",
    "        for model_col in ['xgb_pred','rf_pred','gb_pred','lr_pred','ridge_pred','lasso_pred']:\n",
    "            if model_col in grp.columns:\n",
    "                m = group_metrics(grp, 'actual', model_col)\n",
    "                row[f'{model_col}_RMSE'] = m['RMSE']\n",
    "                row[f'{model_col}_MAE'] = m['MAE']\n",
    "                row[f'{model_col}_R2'] = m['R2']\n",
    "        row['count'] = len(grp)\n",
    "        results.append(row)\n",
    "    res_df_pc = pd.DataFrame(results).sort_values('group')\n",
    "    csv_name_pc = 'spatial_performance_by_passenger_count.csv'\n",
    "    res_df_pc.to_csv(csv_name_pc, index=False)\n",
    "    print(f'Saved {csv_name_pc}')\n",
    "\n",
    "    rmse_cols = [c for c in res_df_pc.columns if c.endswith('_RMSE')]\n",
    "    if len(rmse_cols) > 0:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plot_df = res_df_pc.set_index('group')[rmse_cols]\n",
    "        plot_df.plot(kind='bar', figsize=(10,6))\n",
    "        plt.title('RMSE by Passenger Count')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('Passenger Count')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        img_name_pc = 'spatial_rmse_by_passenger_count.png'\n",
    "        plt.savefig(img_name_pc, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'Saved plot {img_name_pc}')\n",
    "else:\n",
    "    print('passenger_count column not found in test data; skipping passenger_count analysis.')\n",
    "\n",
    "print('\\n✓ Spatial performance analysis complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd57e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 9: VISUALIZATION CONSOLIDATION & COMPARISON PLOTS\n",
      "======================================================================\n",
      "Saved comparison_rmse.png\n",
      "Saved comparison_mae.png\n",
      "Saved comparison_r2.png\n",
      "Saved comparison_mape.png\n",
      "Saved feature_importance_rf_vs_gb.png\n",
      "Saved cv_rmse_boxplot.png\n",
      "Saved pred_vs_actual_xgboost.png\n",
      "Saved pred_vs_actual_random_forest.png\n",
      "Saved pred_vs_actual_gradient_boosting.png\n",
      "\n",
      "✓ Section 9 visualizations created (files saved to working directory).\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 9: VISUALIZATION CONSOLIDATION & COMPARISON PLOTS ====================\n",
    "print('\\n' + '='*70)\n",
    "print('SECTION 9: VISUALIZATION CONSOLIDATION & COMPARISON PLOTS')\n",
    "print('='*70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "# Ensure metrics_df exists\n",
    "if 'metrics_df' not in globals():\n",
    "    raise RuntimeError('metrics_df not found — run Section 4 first to compute metrics.')\n",
    "\n",
    "# 1) Bar charts: RMSE, MAE, R2, MAPE across models\n",
    "metrics_to_plot = ['RMSE','MAE','R2','MAPE']\n",
    "for metric in metrics_to_plot:\n",
    "    try:\n",
    "        vals = metrics_df[metric].dropna()\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {metric}: not available ({e})\")\n",
    "        continue\n",
    "    plt.figure(figsize=(8,5))\n",
    "    vals.sort_values().plot(kind='bar', color='C0')\n",
    "    plt.title(f'{metric} Comparison Across Models')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fname = f'comparison_{metric.lower()}.png'\n",
    "    plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved {fname}\")\n",
    "\n",
    "# 2) Feature importance comparison (RF vs GB) — top 10 union\n",
    "fi_rf = feature_importance_rf.copy() if 'feature_importance_rf' in globals() else None\n",
    "fi_gb = feature_importance_gb.copy() if 'feature_importance_gb' in globals() else None\n",
    "if fi_rf is None and fi_gb is None:\n",
    "    print('No feature importances available (RF/GB missing). Skipping feature importance comparison.')\n",
    "else:\n",
    "    # prepare top features union\n",
    "    top_rf = fi_rf.head(10) if fi_rf is not None else pd.DataFrame(columns=['feature','importance'])\n",
    "    top_gb = fi_gb.head(10) if fi_gb is not None else pd.DataFrame(columns=['feature','importance'])\n",
    "    union_feats = pd.Index(top_rf['feature'].tolist() + top_gb['feature'].tolist()).unique()\n",
    "    comp_df = pd.DataFrame({'feature': union_feats})\n",
    "    if fi_rf is not None:\n",
    "        comp_df = comp_df.merge(fi_rf[['feature','importance']].rename(columns={'importance':'rf_imp'}), on='feature', how='left')\n",
    "    else:\n",
    "        comp_df['rf_imp'] = 0.0\n",
    "    if fi_gb is not None:\n",
    "        comp_df = comp_df.merge(fi_gb[['feature','importance']].rename(columns={'importance':'gb_imp'}), on='feature', how='left')\n",
    "    else:\n",
    "        comp_df['gb_imp'] = 0.0\n",
    "    comp_df = comp_df.fillna(0)\n",
    "    # Normalize for comparison\n",
    "    for col in ['rf_imp','gb_imp']:\n",
    "        if comp_df[col].sum() > 0:\n",
    "            comp_df[col] = comp_df[col] / comp_df[col].sum()\n",
    "    comp_df = comp_df.sort_values(['rf_imp','gb_imp'], ascending=False).set_index('feature')\n",
    "    plt.figure(figsize=(10,6))\n",
    "    comp_df.plot(kind='bar', figsize=(12,6))\n",
    "    plt.title('Feature Importance Comparison (RF vs GB) — normalized')\n",
    "    plt.ylabel('Normalized Importance')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fname = 'feature_importance_rf_vs_gb.png'\n",
    "    plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved {fname}\")\n",
    "\n",
    "# 3) CV RMSE boxplot (from cv_results_df / cv_df)\n",
    "cv_source = None\n",
    "if 'cv_results_df' in globals():\n",
    "    cv_source = cv_results_df.copy()\n",
    "elif 'cv_df' in globals():\n",
    "    cv_source = cv_df.copy()\n",
    "if cv_source is None or cv_source.empty:\n",
    "    print('No CV results found. Skipping CV RMSE boxplot.')\n",
    "else:\n",
    "    # Melt folds into long format for boxplot\n",
    "    cv_long = pd.melt(cv_source, id_vars=['fold'], var_name='model_rmse', value_name='rmse')\n",
    "    # keep only rmse columns\n",
    "    cv_long = cv_long[cv_long['model_rmse'].str.contains('_rmse', case=False)]\n",
    "    # map model labels (xgb_rmse -> XGBoost)\n",
    "    cv_long['model'] = cv_long['model_rmse'].str.replace('_rmse','').str.replace('_',' ').str.title()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x='model', y='rmse', data=cv_long)\n",
    "    plt.title('Cross-Validation RMSE Distribution by Model')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fname = 'cv_rmse_boxplot.png'\n",
    "    plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved {fname}\")\n",
    "\n",
    "# 4) Prediction vs Actual scatter plots for top models (XGBoost, Random Forest, Gradient Boosting if available)\n",
    "top_models = [('XGBoost','y_pred_xgb'), ('Random Forest','y_pred_rf'), ('Gradient Boosting','y_pred_gb')]\n",
    "for label, pred_var in top_models:\n",
    "    if pred_var in globals():\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.scatter(test_analysis['actual'], globals()[pred_var], alpha=0.4, s=10)\n",
    "        plt.plot([test_analysis['actual'].min(), test_analysis['actual'].max()],[test_analysis['actual'].min(), test_analysis['actual'].max()], 'r--')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title(f'Predicted vs Actual — {label}')\n",
    "        plt.tight_layout()\n",
    "        fname = f'pred_vs_actual_{label.lower().replace(\" \",\"_\")}.png'\n",
    "        plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Saved {fname}\")\n",
    "    else:\n",
    "        print(f\"Prediction variable {pred_var} not found — skipping {label} scatter plot.\")\n",
    "\n",
    "print('\\n✓ Section 9 visualizations created (files saved to working directory).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fe95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SECTION 10: MODEL DEPLOYMENT & RECOMMENDATIONS\n",
      "======================================================================\n",
      "\n",
      "--- Model Performance Ranking ---\n",
      "                     RMSE     MAE      R2    MAPE  Overall_Score\n",
      "Gradient Boosting  2.6636  1.6559  0.9379  0.1152            3.0\n",
      "XGBoost            2.7744  1.6614  0.9326  0.1156            7.0\n",
      "Random Forest      2.8280  1.7217  0.9300  0.1205           11.0\n",
      "Lasso Regression   3.7143  2.3554  0.8792  0.1915           15.0\n",
      "Linear Regression  3.7173  2.3575  0.8790  0.1916           21.0\n",
      "Ridge Regression   3.7173  2.3575  0.8790  0.1916           21.0\n",
      "\n",
      "✓ Best Model Selected: Gradient Boosting\n",
      "  RMSE: 2.6636\n",
      "  MAE:  1.6559\n",
      "  R²:   0.9379\n",
      "\n",
      "✓ Best model saved to: gradient_boosting_regressor_phase2.joblib\n",
      "\n",
      "✓ Deployment recommendations saved to: phase2_deployment_recommendations.json\n",
      "\n",
      "======================================================================\n",
      "DEPLOYMENT RECOMMENDATIONS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Selected Model: Gradient Boosting\n",
      "\n",
      "Key Metrics:\n",
      "  RMSE: 2.6636\n",
      "  MAE: 1.6559\n",
      "  R2: 0.9379\n",
      "  MAPE: 0.1152\n",
      "\n",
      "Model Characteristics:\n",
      "  Type: Tree-based Ensemble\n",
      "  Interpretability: Medium (SHAP available)\n",
      "  Training_Speed: Medium\n",
      "  Inference_Speed: Very Fast\n",
      "  Memory_Footprint: Medium\n",
      "\n",
      "Strengths:\n",
      "  • Best overall RMSE: 2.6636\n",
      "  • High R² score: 0.9379\n",
      "  • Robust across cross-validation folds\n",
      "  • Handles non-linear relationships well\n",
      "\n",
      "Weaknesses:\n",
      "  • May overfit on edge cases (very long trips)\n",
      "  • Sensitive to outliers in target variable\n",
      "  • Requires careful feature engineering\n",
      "\n",
      "Deployment Recommendations:\n",
      "  1. Deploy model in staging environment first\n",
      "  2. Monitor prediction errors in production\n",
      "  3. Set up automated retraining pipeline (monthly)\n",
      "  4. Implement feature drift detection\n",
      "  5. Create fallback to baseline model if performance degrades\n",
      "  6. Use SHAP values for prediction explanations\n",
      "  7. Track model performance metrics continuously\n",
      "  8. Establish A/B testing for model updates\n",
      "\n",
      "✓ Final model comparison saved to: phase2_final_model_comparison.csv\n",
      "\n",
      "======================================================================\n",
      "✓ PHASE 2: SUPERVISED MACHINE LEARNING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Timestamp: 2025-12-04 01:14:02\n",
      "\n",
      "Deliverables:\n",
      "  1. phase2_advanced_metrics.json — Comprehensive metrics for all models\n",
      "  2. phase2_cv_results.json — Cross-validation results and fold-wise performance\n",
      "  3. phase2_deployment_recommendations.json — Deployment guidance and recommendations\n",
      "  4. phase2_final_model_comparison.csv — Final comparison table\n",
      "  5. Trained model file — Best model saved for production\n",
      "  6. Visualizations — Multiple PNG files for analysis and reporting\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 10: MODEL DEPLOYMENT & RECOMMENDATIONS ====================\n",
    "print('\\n' + '='*70)\n",
    "print('SECTION 10: MODEL DEPLOYMENT & RECOMMENDATIONS')\n",
    "print('='*70)\n",
    "\n",
    "# 1) Model Performance Summary & Ranking\n",
    "print('\\n--- Model Performance Ranking ---')\n",
    "ranking_df = metrics_df[['RMSE', 'MAE', 'R2', 'MAPE']].copy()\n",
    "ranking_df['Overall_Score'] = (\n",
    "    ranking_df['RMSE'].rank() +\n",
    "    ranking_df['MAE'].rank() +\n",
    "    (6 - ranking_df['R2'].rank()) +\n",
    "    ranking_df['MAPE'].rank()\n",
    ")\n",
    "ranking_df = ranking_df.sort_values('Overall_Score')\n",
    "print(ranking_df.round(4))\n",
    "\n",
    "# 2) Determine Best Model\n",
    "best_model_name = ranking_df.index[0]\n",
    "best_model = None\n",
    "if best_model_name == 'XGBoost':\n",
    "    best_model = xgb_reg\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_reg\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_reg\n",
    "elif best_model_name == 'Linear Regression':\n",
    "    best_model = lr_reg\n",
    "elif best_model_name == 'Ridge Regression':\n",
    "    best_model = ridge_reg\n",
    "elif best_model_name == 'Lasso Regression':\n",
    "    best_model = lasso_reg\n",
    "\n",
    "print(f'\\n✓ Best Model Selected: {best_model_name}')\n",
    "print(f'  RMSE: {metrics_df.loc[best_model_name, \"RMSE\"]:.4f}')\n",
    "print(f'  MAE:  {metrics_df.loc[best_model_name, \"MAE\"]:.4f}')\n",
    "print(f'  R²:   {metrics_df.loc[best_model_name, \"R2\"]:.4f}')\n",
    "\n",
    "# 3) Save Best Model\n",
    "if best_model is not None:\n",
    "    model_filename = f'{best_model_name.lower().replace(\" \", \"_\")}_regressor_phase2.joblib'\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    print(f'\\n✓ Best model saved to: {model_filename}')\n",
    "\n",
    "# 4) Generate Deployment Recommendations Report\n",
    "recommendations = {\n",
    "    'Best_Model': best_model_name,\n",
    "    'Key_Metrics': {\n",
    "        'RMSE': float(metrics_df.loc[best_model_name, 'RMSE']),\n",
    "        'MAE': float(metrics_df.loc[best_model_name, 'MAE']),\n",
    "        'R2': float(metrics_df.loc[best_model_name, 'R2']),\n",
    "        'MAPE': float(metrics_df.loc[best_model_name, 'MAPE'])\n",
    "    },\n",
    "    'Model_Characteristics': {\n",
    "        'Type': 'Tree-based Ensemble' if best_model_name in ['XGBoost', 'Random Forest', 'Gradient Boosting'] else 'Linear',\n",
    "        'Interpretability': 'High' if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression'] else 'Medium (SHAP available)',\n",
    "        'Training_Speed': 'Fast' if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression'] else 'Medium',\n",
    "        'Inference_Speed': 'Very Fast',\n",
    "        'Memory_Footprint': 'Small' if best_model_name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression'] else 'Medium'\n",
    "    },\n",
    "    'Deployment_Readiness': {\n",
    "        'Model_File_Size_MB': os.path.getsize(model_filename) / (1024**2) if best_model is not None and os.path.exists(model_filename) else 'N/A',\n",
    "        'Production_Ready': True,\n",
    "        'Monitoring_Required': True,\n",
    "        'Retraining_Frequency': 'Monthly or upon performance degradation'\n",
    "    },\n",
    "    'Strengths': [\n",
    "        f'Best overall RMSE: {metrics_df.loc[best_model_name, \"RMSE\"]:.4f}',\n",
    "        f'High R² score: {metrics_df.loc[best_model_name, \"R2\"]:.4f}',\n",
    "        'Robust across cross-validation folds',\n",
    "        'Handles non-linear relationships well'\n",
    "    ],\n",
    "    'Weaknesses': [\n",
    "        'May overfit on edge cases (very long trips)',\n",
    "        'Sensitive to outliers in target variable',\n",
    "        'Requires careful feature engineering'\n",
    "    ],\n",
    "    'Recommendations': [\n",
    "        '1. Deploy model in staging environment first',\n",
    "        '2. Monitor prediction errors in production',\n",
    "        '3. Set up automated retraining pipeline (monthly)',\n",
    "        '4. Implement feature drift detection',\n",
    "        '5. Create fallback to baseline model if performance degrades',\n",
    "        '6. Use SHAP values for prediction explanations',\n",
    "        '7. Track model performance metrics continuously',\n",
    "        '8. Establish A/B testing for model updates'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save recommendations to JSON\n",
    "with open('phase2_deployment_recommendations.json', 'w') as f:\n",
    "    json.dump(recommendations, f, indent=4)\n",
    "\n",
    "print('\\n✓ Deployment recommendations saved to: phase2_deployment_recommendations.json')\n",
    "\n",
    "# 5) Print Summary Report\n",
    "print('\\n' + '='*70)\n",
    "print('DEPLOYMENT RECOMMENDATIONS SUMMARY')\n",
    "print('='*70)\n",
    "print(f'\\nSelected Model: {recommendations[\"Best_Model\"]}')\n",
    "print('\\nKey Metrics:')\n",
    "for metric, value in recommendations['Key_Metrics'].items():\n",
    "    print(f'  {metric}: {value:.4f}')\n",
    "print('\\nModel Characteristics:')\n",
    "for char, value in recommendations['Model_Characteristics'].items():\n",
    "    print(f'  {char}: {value}')\n",
    "print('\\nStrengths:')\n",
    "for strength in recommendations['Strengths']:\n",
    "    print(f'  • {strength}')\n",
    "print('\\nWeaknesses:')\n",
    "for weakness in recommendations['Weaknesses']:\n",
    "    print(f'  • {weakness}')\n",
    "print('\\nDeployment Recommendations:')\n",
    "for rec in recommendations['Recommendations']:\n",
    "    print(f'  {rec}')\n",
    "\n",
    "# 6) Create final summary comparison table\n",
    "comparison_summary = pd.DataFrame({\n",
    "    'Model': metrics_df.index,\n",
    "    'RMSE': metrics_df['RMSE'].values,\n",
    "    'MAE': metrics_df['MAE'].values,\n",
    "    'R2': metrics_df['R2'].values,\n",
    "    'MAPE': metrics_df['MAPE'].values,\n",
    "    'Production_Ready': 'Yes'\n",
    "})\n",
    "\n",
    "csv_filename = 'phase2_final_model_comparison.csv'\n",
    "comparison_summary.to_csv(csv_filename, index=False)\n",
    "print(f'\\n✓ Final model comparison saved to: {csv_filename}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('✓ PHASE 2: SUPERVISED MACHINE LEARNING COMPLETE')\n",
    "print('='*70)\n",
    "print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nDeliverables:\")\n",
    "print(\"  1. phase2_advanced_metrics.json — Comprehensive metrics for all models\")\n",
    "print(\"  2. phase2_cv_results.json — Cross-validation results and fold-wise performance\")\n",
    "print(\"  3. phase2_deployment_recommendations.json — Deployment guidance and recommendations\")\n",
    "print(\"  4. phase2_final_model_comparison.csv — Final comparison table\")\n",
    "print(\"  5. Trained model file — Best model saved for production\")\n",
    "print(\"  6. Visualizations — Multiple PNG files for analysis and reporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed0d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
